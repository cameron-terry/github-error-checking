# GitHub Diff Error Checking

A GitHub Actions tool that analyzes pull requests for proper error handling in added code.

This branch is the main branch. All development should be done in feature branches and merged via pull requests.

## How It Works

1. **Identification**: Automatically identifies lines of code that are added in a pull request
2. **Analysis**: Feeds the added code to an LLM Agent for analysis of error handling patterns
3. **Suggestions**: Returns suggestions for improving error handling in the code

## Purpose

This tool helps developers ensure that new code includes proper error handling before it gets merged, reducing the likelihood of production issues caused by unhandled exceptions.

## Installation

To use this GitHub Action in your repository:

1. Create a `.github/workflows/error-check.yml` file with the following content:

```yaml
name: Error Handling Check

on:
  pull_request:
    types: [opened, synchronize, reopened]

jobs:
  check-error-handling:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Check for error handling
        uses: cameronterry/github-diff-error-checking@v1
        id: error-check
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          openai-api-key: ${{ secrets.OPENAI_API_KEY }}
          llm-model: gpt-3.5-turbo  # Optional: Set the LLM model
          log-level: info  # Optional: Set the logging level
          
      - name: Report results
        run: echo "Found ${{ steps.error-check.outputs.added-code }} sections with an average score of ${{ steps.error-check.outputs.error-score }}/10"
```

2. Add your OpenAI API key to GitHub Secrets (see [Setting Up API Key](#setting-up-api-key))

## Setting Up API Key

This action requires an OpenAI API key to analyze code for error handling issues. To set up your API key:

1. Obtain an API key from [OpenAI](https://platform.openai.com/api-keys)
2. In your GitHub repository, go to **Settings** → **Secrets and variables** → **Actions**
3. Click **New repository secret**
4. Name: `OPENAI_API_KEY`
5. Value: Your OpenAI API key
6. Click **Add secret**

The action will automatically use this secret to authenticate with OpenAI's API for code analysis.

## Configuration

### Input

| Input | Description | Required | Default |
|-------|-------------|----------|---------|
| `github-token` | GitHub token for API access | Yes | `${{ github.token }}` |
| `error-types` | Types of error handling to check for (comma-separated) | No | `exceptions,null-checks,undefined-checks` |
| `openai-api-key` | OpenAI API key for analyzing code | Yes | - |
| `llm-model` | LLM model to use for analysis | No | `gpt-4` |
| `log-level` | Logging level (debug, info, warning, error, none) | No | `info` |
| `enable-file-filtering` | Enable filtering out common generated/config files | No | `true` |
| `ignore-patterns` | Additional file patterns to ignore (comma-separated) | No | `''` |

### Output

| Output | Description |
|--------|-------------|
`added-code` | Number of code sections found for analysis
`analysis-results` | JSON string containing analysis results with issues and suggestions
`error-score` | Average score (0-10) for error handling quality across all sections

### File Filtering

The action can automatically skip analysis for common autogenerated files and configuration files that typically don't require error handling analysis. This helps focus the analysis on actual application code and reduces token usage.

By default, the following types of files are excluded:
- Configuration files (`.json`, `.eslintrc`, etc.)
- Source maps (`.map` files)
- TypeScript declaration files (`.d.ts`)
- Minified files (`.min.js`, `.min.css`)
- Build outputs (`dist/`, `build/`, etc.)
- Package management files (`package.json`, `package-lock.json`, etc.)
- Documentation (`.md`, `docs/`, etc.)

You can:
- Disable file filtering completely with `enable-file-filtering: 'false'`
- Add your own patterns to ignore with `ignore-patterns: 'vendor/,generated/,etc/'`

```yaml
- name: Check for error handling
  uses: cameronterry/github-diff-error-checking@v1
  with:
    github-token: ${{ secrets.GITHUB_TOKEN }}
    openai-api-key: ${{ secrets.OPENAI_API_KEY }}
    enable-file-filtering: 'false'  # Analyze all files, including generated ones
    ignore-patterns: '.generated.ts,.pb.go,vendor/'  # Add custom patterns to ignore
```

When running locally, you can also configure file filtering via environment variables:
```bash
export ENABLE_FILE_FILTERING=false
export IGNORE_PATTERNS=vendor/,generated/
node lib/src/index.js samples/axios.diff
```

You can also set the log level via environment variable:

```bash
export LOG_LEVEL=debug
node lib/src/index.js samples/axios.diff
```

## Outputs

| Output | Description |
|--------|-------------|
| `added-code` | Number of code sections found for analysis |
| `analysis-results` | JSON array of analysis results with issues and suggestions |
| `error-score` | Average score (0-10) for error handling quality across all sections |

## Analysis Results Format

The action outputs a detailed JSON array of analysis results in the `analysis-results` output. Each item in the array represents a file that was analyzed and includes:

- `file`: Path to the analyzed file
- `issues`: Array of identified issues, each containing:
  - `description`: Description of the error handling issue
  - `suggestion`: Recommended fix for the issue
  - `severity`: Issue severity level ("high", "medium", or "low")
  - `lineNumber`: Line number where the issue was found
- `score`: Overall error handling score for the file (0-10)

### Example Analysis Results

```json
[
  {
    "file": "src/diff-utils.ts",
    "issues": [
      {
        "description": "Missing exception handling for API call",
        "suggestion": "Wrap the API call with a try/catch block to properly handle potential errors",
        "severity": "high",
        "lineNumber": 6
      },
      {
        "description": "Improper error propagation",
        "suggestion": "Instead of throwing a generic error message, handle specific error types and messages for better debugging and error tracing",
        "severity": "medium",
        "lineNumber": 18
      }
    ],
    "score": 7
  },
  {
    "file": "src/file-filters.ts",
    "issues": [
      {
        "description": "Missing exception handling for core.getInput in getAdditionalIgnorePatterns function",
        "suggestion": "Add try/catch block around core.getInput(ignore-patterns, { required: false }) to handle potential errors",
        "severity": "medium",
        "lineNumber": 36
      }
    ],
    "score": 7
  }
]
```

## Development

### Prerequisites

- Node.js 14 or later
- npm or yarn
- OpenAI API key for testing LLM integration

### Setting up the development environment

1. Clone the repository
2. Install dependencies:
   ```
   npm install
   ```
3. Start TypeScript in watch mode during development:
   ```
   npm run dev
   ```
4. Build the project:
   ```
   npm run build
   ```
5. Run tests:
   ```
   npm test
   ```
6. Run linting:
   ```
   npm run lint
   ```
7. Type-check without emitting files:
   ```
   npm run typecheck
   ```

### Local Testing

You can test the diff parsing and LLM analysis functionality locally without creating a real PR:

1. Use one of the sample diff files in the `samples/` directory
2. Set your OpenAI API key in the environment:
   ```
   export OPENAI_API_KEY=your_api_key_here
   ```
3. Run the local test runner:
   ```
   node lib/index.js samples/axios.diff
   ```
   or for a more complex example:
   ```
   node lib/index.js samples/user-order-services.diff
   ```

This will show you the added code sections and their LLM analysis results for error handling.

See the [samples/README.md](samples/README.md) for more information about the sample files and the error handling issues they contain.

### Project Structure

- `src/index.ts` - Main entry point for the GitHub Action
- `src/diff-utils.ts` - Utilities for parsing PR diffs and extracting added code
- `src/llm-service.ts` - Service for LLM-based code analysis
- `src/logger.ts` - Logging utility with support for different verbosity levels
- `tests/` - Test files
- `lib/` - Compiled JavaScript output from TypeScript
- `.github/workflows/` - GitHub workflows for testing the action
- `samples/` - Sample diff files for testing
- `index.ts` - Local test runner for development
- `.eslintrc.js` - ESLint configuration
- `jest.config.js` - Jest test configuration
- `tsconfig.json` - TypeScript configuration
- `build.js` - Script to compile the action using tsc and ncc

### Dependencies

- `@actions/core` and `@actions/github` - Core GitHub Actions libraries
- `parse-diff` - Library to parse git diff output
- `openai` - OpenAI API client for LLM integration
- TypeScript support: `typescript`, `@types/node`, `@types/jest`
- Development tools: Jest with `ts-jest`, ESLint with TypeScript plugins, `@vercel/ncc`

## Implementation Details

### Step 1: Identifying Added Code

The first step of the process identifies lines that have been added in a pull request:

1. Retrieves the PR diff using the GitHub API
2. Parses the diff to extract added lines of code
3. Maintains context (lines before and after) for proper analysis
4. Outputs structured data for further analysis
5. Differentiates between new code blocks and modifications to existing code

### Step 2: LLM Analysis

This step analyzes the added code for proper error handling patterns using LLM technology:

1. Prepares each code section with relevant context
2. Sends the code to OpenAI's API with a specialized prompt
3. Analyzes the code for missing exception handling, null checks, and other error handling patterns
4. Returns structured feedback with severity ratings and suggestions
5. Assigns an overall score (0-10) for error handling quality

### Step 3: Suggestions

The final step provides actionable suggestions for improving error handling:

1. Groups related issues for better clarity
2. Provides specific code suggestions for fixing each issue
3. Highlights critical issues that require immediate attention
4. Gives an overall assessment of error handling quality

## Type Safety

The project is fully typed with TypeScript, providing:

- Explicit interfaces for all data structures
- Type validation for GitHub API calls and responses
- Better IDE support with autocompletion and error detection
- More reliable code through compile-time type checking

## Status

🚧 Under Development 🚧

Currently implemented:
- Step 1: Identifying added lines of code in a PR
- Step 2: LLM analysis of code for error handling issues
- Development environment with local testing capability
- Sample diff files with various error handling patterns
- Complete TypeScript conversion for type safety
- Structured logging system with configurable verbosity levels 